{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1024)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,361,096\n",
      "Trainable params: 132,232\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Initialize MobileNet pre-trained on ImageNet\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze all layers in the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build the custom classification model\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)  # Keep MobileNet in inference mode\n",
    "x = layers.GlobalAveragePooling2D()(x)  # Convert feature maps to a single vector\n",
    "x = layers.Dense(128, activation='relu')(x)  # Fully connected layer\n",
    "x = layers.Dropout(0.5)(x)  # Reduce overfitting\n",
    "outputs = layers.Dense(8, activation='softmax')(x)  # 8 output classes\n",
    "\n",
    "# Define model\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_images_and_labels(directory, img_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Load images and labels from a dataset directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the dataset directory (train, test, val).\n",
    "        img_size (tuple): Target size for image resizing (default: (224, 224)).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed images.\n",
    "        np.ndarray: Corresponding labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = os.listdir(directory)\n",
    "    class_mapping = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_path):  # Ensure it's a directory\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, img_size)\n",
    "                    images.append(img)\n",
    "                    labels.append(class_mapping[class_name])\n",
    "\n",
    "    images = np.array(images, dtype=np.float32) / 255.0  # Normalize images\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 319 samples\n",
      "Validation data: 357 samples\n"
     ]
    }
   ],
   "source": [
    "# Define paths to datasets\n",
    "train_path = '/Users/shubhampund9767/Documents/Cotton-Disease-Recognition-using-YOLO-Algorithm--1/Cotton-Disease-Specific/trainning/images'\n",
    "test_path = '/content/drive/MyDrive/Cotton Disease/test'\n",
    "val_path = '/Users/shubhampund9767/Documents/Cotton-Disease-Recognition-using-YOLO-Algorithm--1/Cotton-Disease-Specific/validation/images'\n",
    "\n",
    "# Load datasets\n",
    "X_train, y_train = load_images_and_labels(train_path)\n",
    "\n",
    "X_val, y_val = load_images_and_labels(val_path)\n",
    "\n",
    "print(f\"Training data: {len(X_train)} samples\")\n",
    "\n",
    "print(f\"Validation data: {len(X_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (319, 8)\n",
      "Validation labels shape: (357, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# One-hot encode labels\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "\n",
    "y_val = lb.transform(y_val)\n",
    "\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "\n",
    "print(f\"Validation labels shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 5s 493ms/step - loss: 0.1141 - accuracy: 0.9749 - val_loss: 7.3881 - val_accuracy: 0.1092\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 5s 482ms/step - loss: 0.1141 - accuracy: 0.9749 - val_loss: 7.6354 - val_accuracy: 0.1092\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 5s 489ms/step - loss: 0.1061 - accuracy: 0.9812 - val_loss: 7.6546 - val_accuracy: 0.1092\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 4s 457ms/step - loss: 0.0804 - accuracy: 0.9843 - val_loss: 7.8139 - val_accuracy: 0.1092\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 5s 487ms/step - loss: 0.0631 - accuracy: 0.9937 - val_loss: 8.2341 - val_accuracy: 0.1092\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 4s 464ms/step - loss: 0.0829 - accuracy: 0.9843 - val_loss: 8.3822 - val_accuracy: 0.1092\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 5s 490ms/step - loss: 0.0554 - accuracy: 0.9937 - val_loss: 8.5133 - val_accuracy: 0.1092\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 5s 488ms/step - loss: 0.0635 - accuracy: 0.9875 - val_loss: 8.6035 - val_accuracy: 0.1092\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 5s 534ms/step - loss: 0.0601 - accuracy: 0.9843 - val_loss: 8.6988 - val_accuracy: 0.1092\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 5s 510ms/step - loss: 0.0378 - accuracy: 0.9937 - val_loss: 8.7922 - val_accuracy: 0.1092\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 5s 502ms/step - loss: 0.0416 - accuracy: 0.9906 - val_loss: 8.9986 - val_accuracy: 0.1092\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 5s 518ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 9.2754 - val_accuracy: 0.1092\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 5s 531ms/step - loss: 0.0374 - accuracy: 0.9843 - val_loss: 9.5363 - val_accuracy: 0.1092\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 5s 527ms/step - loss: 0.0404 - accuracy: 0.9906 - val_loss: 9.4923 - val_accuracy: 0.1092\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 5s 528ms/step - loss: 0.0362 - accuracy: 0.9937 - val_loss: 9.4094 - val_accuracy: 0.1092\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 5s 531ms/step - loss: 0.0407 - accuracy: 0.9937 - val_loss: 9.4395 - val_accuracy: 0.1092\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 5s 550ms/step - loss: 0.0294 - accuracy: 0.9937 - val_loss: 9.4168 - val_accuracy: 0.1092\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 5s 528ms/step - loss: 0.0296 - accuracy: 0.9969 - val_loss: 9.5341 - val_accuracy: 0.1092\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 5s 545ms/step - loss: 0.0323 - accuracy: 0.9937 - val_loss: 9.6473 - val_accuracy: 0.1092\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 5s 538ms/step - loss: 0.0319 - accuracy: 0.9937 - val_loss: 9.8273 - val_accuracy: 0.1092\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 5s 546ms/step - loss: 0.0314 - accuracy: 0.9906 - val_loss: 9.9705 - val_accuracy: 0.1092\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 5s 542ms/step - loss: 0.0301 - accuracy: 0.9937 - val_loss: 10.0478 - val_accuracy: 0.1092\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 5s 545ms/step - loss: 0.0199 - accuracy: 0.9969 - val_loss: 10.0345 - val_accuracy: 0.1092\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 5s 540ms/step - loss: 0.0217 - accuracy: 0.9969 - val_loss: 10.2424 - val_accuracy: 0.1092\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 5s 549ms/step - loss: 0.0247 - accuracy: 0.9969 - val_loss: 10.2514 - val_accuracy: 0.1092\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 5s 564ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 10.1186 - val_accuracy: 0.1092\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 6s 635ms/step - loss: 0.0210 - accuracy: 0.9969 - val_loss: 10.0415 - val_accuracy: 0.1092\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 6s 662ms/step - loss: 0.0142 - accuracy: 0.9969 - val_loss: 10.2246 - val_accuracy: 0.1092\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 6s 600ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 10.3637 - val_accuracy: 0.1092\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 5s 573ms/step - loss: 0.0263 - accuracy: 0.9937 - val_loss: 10.5443 - val_accuracy: 0.1092\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 6s 583ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 10.7020 - val_accuracy: 0.1120\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 6s 587ms/step - loss: 0.0214 - accuracy: 0.9969 - val_loss: 10.9790 - val_accuracy: 0.1092\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 5s 568ms/step - loss: 0.0240 - accuracy: 0.9906 - val_loss: 11.0709 - val_accuracy: 0.1092\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 5s 553ms/step - loss: 0.0370 - accuracy: 0.9906 - val_loss: 11.0401 - val_accuracy: 0.1120\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 5s 557ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: 10.4732 - val_accuracy: 0.1092\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 5s 562ms/step - loss: 0.0227 - accuracy: 0.9937 - val_loss: 10.2540 - val_accuracy: 0.1120\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 5s 562ms/step - loss: 0.0208 - accuracy: 1.0000 - val_loss: 10.4925 - val_accuracy: 0.1092\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 5s 575ms/step - loss: 0.0153 - accuracy: 0.9969 - val_loss: 10.7380 - val_accuracy: 0.1092\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 5s 571ms/step - loss: 0.0151 - accuracy: 0.9969 - val_loss: 10.8485 - val_accuracy: 0.1092\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 5s 560ms/step - loss: 0.0185 - accuracy: 0.9969 - val_loss: 10.9143 - val_accuracy: 0.1092\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 6s 595ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 10.8759 - val_accuracy: 0.1092\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 6s 597ms/step - loss: 0.0145 - accuracy: 0.9969 - val_loss: 10.9369 - val_accuracy: 0.1120\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 6s 599ms/step - loss: 0.0139 - accuracy: 0.9969 - val_loss: 10.9873 - val_accuracy: 0.1120\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 6s 586ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 11.2262 - val_accuracy: 0.1120\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 11.4014 - val_accuracy: 0.1120\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 6s 597ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 11.4801 - val_accuracy: 0.1092\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 6s 587ms/step - loss: 0.0187 - accuracy: 0.9969 - val_loss: 11.6237 - val_accuracy: 0.1092\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 6s 591ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 11.5838 - val_accuracy: 0.1092\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 6s 604ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 11.5647 - val_accuracy: 0.1092\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 6s 641ms/step - loss: 0.0108 - accuracy: 0.9969 - val_loss: 11.6319 - val_accuracy: 0.1092\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,  # Adjust epochs as needed\n",
    "    validation_data=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"new_cotton_disease_model.h5\")  # Saves as a directory with all required files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"/Users/shubhampund9767/Documents/Cotton-Disease-Recognition-using-YOLO-Algorithm--1/new_cotton_disease_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "def predict_image(image_path, class_names):\n",
    "    \"\"\"\n",
    "    Predicts the class of an image using the trained model.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image to predict.\n",
    "        class_names (list): List of class labels.\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted class label.\n",
    "        float: Confidence score of the prediction.\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    img = load_img(image_path, target_size=(224, 224))  # Replace with your model's input size\n",
    "    img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array)\n",
    "    print(f\"Raw predictions: {predictions}\")  # Debugging output\n",
    "\n",
    "    # Get the predicted class index and confidence\n",
    "    predicted_index = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_index]\n",
    "\n",
    "    # Check if predicted index is valid\n",
    "    if predicted_index >= len(class_names):\n",
    "        raise ValueError(\n",
    "            f\"Predicted index {predicted_index} exceeds the number of class labels in class_names {len(class_names)}.\"\n",
    "        )\n",
    "\n",
    "    # Map predicted index to class label\n",
    "    predicted_class = class_names[predicted_index]\n",
    "\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "Raw predictions: [[9.6944696e-01 1.1075893e-02 3.3254514e-03 2.0897659e-04 1.3207029e-04\n",
      "  2.9823869e-03 7.5654555e-03 5.2627702e-03]]\n",
      "Predicted Class: Aphids\n",
      "Confidence Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "class_names =['Aphids', 'Army worm', 'Bacterial blight', 'Cotton Boll Rot','Green Cotton Boll' ,\n",
    "        'Healthy', 'Powdery mildew', 'Target spot' ]\n",
    "\n",
    "# Example: Predict for a sample image\n",
    "image_path = \"/Users/shubhampund9767/Documents/Cotton-Disease-Recognition-using-YOLO-Algorithm--1/cotton.jpeg\"\n",
    "predicted_class, confidence = predict_image(image_path, class_names)\n",
    "\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Confidence Score: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
